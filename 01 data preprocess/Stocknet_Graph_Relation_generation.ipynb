{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lmSmsrD5jYcU"
      },
      "outputs": [],
      "source": [
        "!pip install -U -q PyDrive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-z5uwBIdjXOn",
        "outputId": "1a08f0c3-c0b1-4bf2-d124-f5485d129694"
      },
      "outputs": [],
      "source": [
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        " \n",
        "# 1. Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "file_list = drive.ListFile({'q': \"'1DIF8C7O9SUS3puW3NHXymKa2Nz3GG8L7' in parents and trashed=false\"}).GetList()\n",
        "for file1 in file_list:\n",
        "    if file1['title'] == 'wikidata_raw.zip':\n",
        "        preprocessorFile = drive.CreateFile({'id': file1['id']})\n",
        "        preprocessorFile.GetContentFile('wikidata_raw.zip')\n",
        "        print('title: %s, id: %s is downloaded' % (file1['title'], file1['id']))\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N1a578lfjmMb",
        "outputId": "b78414e8-e884-424b-bded-4eafb5405702"
      },
      "outputs": [],
      "source": [
        "!unzip wikidata_raw.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DQ4Ke7iYjzIL"
      },
      "outputs": [],
      "source": [
        "!rm wikidata_raw.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ogjFLdzdkIZ8"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-1zQMbxGdI-j"
      },
      "outputs": [],
      "source": [
        "wikidata_filepath = 'wikidata_raw'\n",
        "wikidata_entries_filepath = os.path.join(wikidata_filepath, 'wikidata_entries')\n",
        "# Entity numbers\n",
        "# NOTE: SPLP could not be found on Wikidata\n",
        "name_to_num = {}\n",
        "num_to_name = {}\n",
        "entities = set()\n",
        "with open(os.path.join(wikidata_filepath, 'links.csv')) as file:\n",
        "    next(file)\n",
        "    for line in file:\n",
        "        name, url = line.split(',')\n",
        "        entity_num = int(re.sub('\\D', '', url.split(',')[-1]))\n",
        "        name_to_num[name] = entity_num\n",
        "        num_to_name[entity_num] = name\n",
        "        entities.add(entity_num)\n",
        "    # Building first-order relationships\n",
        "    graph = {}\n",
        "    for name in name_to_num.keys():\n",
        "        graph[name] = []\n",
        "    graph['SPLP'] = []\n",
        "\n",
        "for filename in os.listdir(wikidata_entries_filepath):\n",
        "    with open(os.path.join(wikidata_entries_filepath, filename)) as file:\n",
        "        if filename == '.DS_Store': continue\n",
        "            \n",
        "        orig_entity = filename.split('_')[0]\n",
        "        # print('\\nSearching entity', orig_entity)\n",
        "        \n",
        "        for e_id in entities:\n",
        "            file.seek(0, 0)\n",
        "            if str(e_id) in file.read():\n",
        "                # print(f'{orig_entity} contains entity {num_to_name[e_id]}')\n",
        "                graph[orig_entity].append(num_to_name[e_id])\n",
        "\n",
        "# Make graph non-directional\n",
        "for co1 in graph.keys():\n",
        "    co2s = graph[co1]\n",
        "    for co2 in co2s:\n",
        "        if co1 not in graph[co2]:\n",
        "            graph[co2].append(co1)\n",
        "\n",
        "# Build graph mapping companies to all their related entities\n",
        "entity_regex = re.compile(\".+Q[0-9].+\")\n",
        "\n",
        "company_to_entities = {}\n",
        "for name in name_to_num.keys():\n",
        "    company_to_entities[name] = []\n",
        "company_to_entities['SPLP'] = []\n",
        "\n",
        "for filename in os.listdir(wikidata_entries_filepath):\n",
        "    with open(os.path.join(wikidata_entries_filepath, filename)) as file:\n",
        "        if filename == '.DS_Store': continue\n",
        "            \n",
        "        orig_entity = filename.split('_')[0]\n",
        "        # print('\\nSearching entity', orig_entity, name_to_num[orig_entity])\n",
        "        \n",
        "        for line in file:\n",
        "            if entity_regex.match(line):\n",
        "                try:\n",
        "                    q_index = line.index('Q')\n",
        "                    s_index = line[q_index:].index(' ') + q_index\n",
        "                    related_entity = line[q_index + 1:s_index]\n",
        "                    \n",
        "                    if '-' not in related_entity:\n",
        "                        # print('>', related_entity)\n",
        "                        if related_entity not in company_to_entities[orig_entity]:\n",
        "                            company_to_entities[orig_entity].append(related_entity)\n",
        "                except ValueError:\n",
        "                    # print('substring err')\n",
        "                    pass\n",
        "\n",
        "# Build second-order relations\n",
        "graph_2 = {}\n",
        "for name in name_to_num.keys():\n",
        "    graph_2[name] = set()\n",
        "graph_2['SPLP'] = set()\n",
        "\n",
        "def common_member(a, b): \n",
        "    a_set = set(a) \n",
        "    b_set = set(b) \n",
        "    if len(a_set.intersection(b_set)) > 0: \n",
        "        return(True)  \n",
        "    return(False)  \n",
        "\n",
        "for company in company_to_entities.keys():\n",
        "    for other_company in company_to_entities.keys():\n",
        "        if common_member(company_to_entities[company], company_to_entities[other_company]):\n",
        "            graph_2[company].add(other_company)\n",
        "\n",
        "# Make graph non-directional\n",
        "for co1 in graph_2.keys():\n",
        "    co2s = graph_2[co1]\n",
        "    for co2 in co2s:\n",
        "        if co1 not in graph_2[co2]:\n",
        "            graph[co2].add(co1)\n",
        "\n",
        "# First and second order graphs combined\n",
        "graph_1_2 = {}\n",
        "for name in name_to_num.keys():\n",
        "    graph_1_2[name] = set()\n",
        "graph_1_2['SPLP'] = set()\n",
        "\n",
        "for company in graph_1_2.keys():\n",
        "    graph_1_2[company].update(graph[company])\n",
        "    graph_1_2[company].update(graph_2[company])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-jBq_A6ptWo_",
        "outputId": "22d0dbdb-6d69-4a18-d37f-74584ab9ba8b"
      },
      "outputs": [],
      "source": [
        "for file1 in file_list:\n",
        "    if file1['title'] == 'tweet_output.zip':\n",
        "        preprocessorFile = drive.CreateFile({'id': file1['id']})\n",
        "        preprocessorFile.GetContentFile('tweet_output.zip')\n",
        "        print('title: %s, id: %s is downloaded' % (file1['title'], file1['id']))\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "meEZu6JgteKm",
        "outputId": "0c181f42-47b7-439e-d399-1300530302e7"
      },
      "outputs": [],
      "source": [
        "!unzip tweet_output.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ize3YIT8thJM"
      },
      "outputs": [],
      "source": [
        "!rm tweet_output.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "27TqaC5vuBWU"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xH61gXastwLP"
      },
      "outputs": [],
      "source": [
        "with open('tweet_output/stock_key_seq.np','rb') as f:\n",
        "    tweetseq = np.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Fy8jjUR_Svu"
      },
      "outputs": [],
      "source": [
        "output = np.empty((0,88))\n",
        "for i in tweetseq:\n",
        "    temp = np.array([0]*88)\n",
        "    for j in graph_1_2[i]:\n",
        "        out=np.where(tweetseq == j)\n",
        "        temp[out[0][0]] = 1\n",
        "    output=np.vstack((output, temp))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "thflBK3LAUcy"
      },
      "outputs": [],
      "source": [
        "with open('tweet_output/graph_relation.npy','wb') as f:\n",
        "    np.save(f, output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IrZvDsYUC5zv",
        "outputId": "d8612694-7c9a-4fba-99d3-7c0683e85e6d"
      },
      "outputs": [],
      "source": [
        "!zip -r tweet_output.zip tweet_output"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Stocknet Graph Relation generation.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "5c0372ed38b372118c24adb00d45654d76c8d10261533c5724e3f5fc1d75489a"
    },
    "kernelspec": {
      "display_name": "Python 3.8.6 64-bit",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": ""
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}