# -*- coding: utf-8 -*-
"""Deep Attentive Learning tweet pre-process

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1FWDmPMGsihaU7sgzZuCBEOF0Qyobef0s
"""
import os
import json
import tensorflow_hub as hub
import pandas as pd
from tqdm import tqdm
import numpy as np
import pickle

Basename = os.path.dirname(os.path.abspath(__file__))
dataset_dir = os.path.join(Basename, '01 data preprocess')

embed = hub.load("https://tfhub.dev/google/universal-sentence-encoder/4")

for fname in ['train_timestamp.csv', 'valid_timestamp.csv', 'test_timestamp.csv']:
    timestamps_df = pd.read_csv(os.path.join(dataset_dir, fname))
    tweets_text = {}
    print(f'{fname} in process', flush=True)
    for allow_date in tqdm(timestamps_df['Date'].values):
        tweets_text[allow_date] = {}
        for stock_name in sorted(os.listdir(f'{dataset_dir}/preprocessed')):
            if allow_date in sorted(os.listdir(f'{dataset_dir}/preprocessed/{stock_name}')):
                with open(f'{dataset_dir}/preprocessed/{stock_name}/{allow_date}') as f:
                    lines = f.readlines()
                    tweets = [' '.join(json.loads(tweet).get('text')) for tweet in lines]
                embeddings = embed(tweets)
                tweets_text[allow_date][stock_name] = embeddings.numpy()
            else:
                tweets_text[allow_date][stock_name] = np.array([])
    print((len(tweets_text.keys()), len(tweets_text[list(tweets_text.keys())[0]])))
    root = os.path.join(dataset_dir, 'tweet_output')
    if not os.path.exists(root):
        os.mkdir(root)
    filename = os.path.join(root, fname.split('_')[0] + '_tweet.pkl')
    with open(filename, 'wb') as output:
        pickle.dump(tweets_text, output, pickle.HIGHEST_PROTOCOL)
